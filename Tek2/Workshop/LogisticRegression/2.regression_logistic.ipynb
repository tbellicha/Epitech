{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mindset d'un réseau de neurones avec la régression logistique\n",
    "\n",
    "Dans ce workshop, vous allez coder un réseau de neurones pour reconnaître des chats à partir d'une image. Vous apprendrez le mindset (= l'état d'esprit) du fonctionnement d'un réseau de neurones, et acquérir, de manière générale, une idée de ce qu'est le deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- N'utilisez pas de boucles for ou while à moins qu'on ne vous le demande explicitement.\n",
    "\n",
    "\n",
    "**Vous allez apprendre à:**\n",
    "- Construire l'architecture générale d'un modèle d'apprentissage incluant:\n",
    "    - l'initialisation des paramètres\n",
    "    - le calcul de la fonction de coût et de son gradient\n",
    "    - l'utilisation d'un algorithme d'optimisation\n",
    "- Regrouper les trois fonctions ci-dessus pour le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Importons d'abord les packages suivant dont vous aurez besoin:\n",
    "- [numpy](www.numpy.org) est le package fondamental pour le calcul scientifique avec Python.\n",
    "- [h5py](http://www.h5py.org) est un package permettant d'intéragir avec un jeu de données stocké dans un fichier H5.\n",
    "- [matplotlib](http://matplotlib.org) est une bibliothèque connue pour afficher des graphiques en Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) et [scipy](https://www.scipy.org/) sont utilisés ici pour tester votre modèle avec vos propres photos à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Overview de la problématique##\n",
    "\n",
    "**Problématique**: \n",
    "Nous vous avons donné un jeu de données (\"data.h5\") contenant:\n",
    "    - un training set de m_train images, chacune labellisées comme étant un chat (y=1) ou non-chat(y=0)\n",
    "    - un test set de m_test images, chacune labellisées comme étant un chat ou non-chat\n",
    "    - chaque image est de la forme (num_px, num_px, 3) où 3 correspond aux trois canaux RGB. Chaque image est donc carrée de côté num_px.\n",
    "\n",
    "Vous allez construire un algorithme simple de reconnaissance d'images qui pourra classifier correctement les images de chats des autres.\n",
    "\n",
    "Explorons d'abord notre jeu de données. Commençons par l'importer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "x_train_orig, y_train, x_test_orig, y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ajouté \"_orig\" à la fin des jeux de données d'images (train et test) car nous allons par la suite les traiter. En général, lorque l'on vous donne un jeu de données, il n'est jamais parfait. Vous devrez toujours passer par une étape de `nettoyage` appelée `préprocessing`. Après cette étape, vous vous retrouverez avec x_train et x_test (y_train et y_test n'ont pas besoin de préprocessing)\n",
    "\n",
    "Chaque ligne de votre x_train_orig et y_test_orig est un tableau représentant une image. Vous pouvez en visualiser une en exécutant le code suivant. Vous pouvez aussi changer la valeur de `index` si vous voulez voir d'autres images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [0], ceci n'est pas une photo de chat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemple d'une image\n",
    "index = 30\n",
    "plt.imshow(x_train_orig[index])\n",
    "if classes[np.squeeze(y_train[:, index])] == b'cat':\n",
    "    print(\"y = \" + str(y_train[:, index]) + \", ceci est une photo de chat\")\n",
    "else:\n",
    "    print(\"y = \" + str(y_train[:, index]) + \", ceci n'est pas une photo de chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de bugs en deep learning proviennent du fait que les dimensions de matrices/vecteurs sont incorrectes. En y faisant attention, vous éviterez de nombreux bugs.\n",
    "\n",
    "\n",
    "**Exercise:** Trouvez les bonnes valeurs de:\n",
    "    - m_train (nombre d'images de train)\n",
    "    - m_test (nombre d'images de test)\n",
    "    - num_px (longueur/largeur d'une image)\n",
    "    \n",
    "Rappelez vous que x_train_orig est un numpy array de la forme (m_train, num_px, num_px, 3). Par exemple, vous pouvez accéder à `m_train` en écrivant `x_train_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples de train: m_train = 209\n",
      "Nombre d'exemples de test: m_test = 50\n",
      "Longueur/Largeur de chaque image: num_px = 64\n",
      "Chaque image est de size: (64, 64, 3)\n",
      "x_train shape: (209, 64, 64, 3)\n",
      "y_train shape: (1, 209)\n",
      "x_test shape: (50, 64, 64, 3)\n",
      "y_test shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### Début du code ### (≈ 3 lignes de code)\n",
    "m_train = x_train_orig.shape[0]\n",
    "m_test = x_test_orig.shape[0]\n",
    "num_px = x_train_orig.shape[2]\n",
    "### Fin du code ###\n",
    "\n",
    "print (\"Nombre d'exemples de train: m_train = \" + str(m_train))\n",
    "print (\"Nombre d'exemples de test: m_test = \" + str(m_test))\n",
    "print (\"Longueur/Largeur de chaque image: num_px = \" + str(num_px))\n",
    "print (\"Chaque image est de size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"x_train shape: \" + str(x_train_orig.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"x_test shape: \" + str(x_test_orig.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attentu pour m_train, m_test and num_px**: \n",
    "\n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>m_train</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des raisons pratiques, vous devriez transformer vos images de la forme (num_px, num_px, 3) en un numpy array de la forme (num_px $*$ num_px $*$ 3, 1). Après cela, nos jeux de données (train et test) seront des numpy array dans laquelle chaque colonne représente une image applatie (\"flatten\" en anglais). Il devrait y avoir m_train et m_test colonnes.\n",
    "\n",
    "\n",
    "**Exercise:** \n",
    "Transformer les jeux de données de train et de test de façon à ce que les images de la forme (num_px, num_px, 3) soient applatie en simples vecteurs de la forme (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "Petite astuce: lorsque vous voulez applatir une matrice X de la forme (a,b,c,d) en une matrice X_flatten de la forme (b$*$c$*$d, a): \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T est la transposée de X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_flatten shape: (12288, 209)\n",
      "y_train shape: (1, 209)\n",
      "x_test_flatten shape: (12288, 50)\n",
      "y_test shape: (1, 50)\n",
      "check 1 random après reshaping: [182 188 179 174 213]\n",
      "check 2 random après reshaping: [20 16  3 22 15]\n"
     ]
    }
   ],
   "source": [
    "### Début du code ### (≈ 2 lignes de code)\n",
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n",
    "### Fin du code ###\n",
    "\n",
    "print (\"x_train_flatten shape: \" + str(x_train_flatten.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"x_test_flatten shape: \" + str(x_test_flatten.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))\n",
    "print (\"check 1 random après reshaping: \" + str(x_train_flatten[5:10,1]))\n",
    "print (\"check 2 random après reshaping: \" + str(x_train_flatten[17:22,34]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "    x_train_flatten shape: (12288, 209)\n",
    "    y_train shape: (1, 209)\n",
    "    x_test_flatten shape: (12288, 50)\n",
    "    y_test shape: (1, 50)\n",
    "    check 1 random après reshaping: [182 188 179 174 213]\n",
    "    check 2 random après reshaping: [20 16  3 22 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour représenter des images en couleurs, les canaux rouges, verts et bleus (RGB) doivent être spécifiés pour chaque pixel, et la valeur de chaque pixel correspond en fait à un vecteur de 3 nombres compris entre 0 et 255.\n",
    "\n",
    "Une étape de préprocessing assez courante en machine learning est de centrer et normaliser votre jeu de données, ce qui signifie que vous allez calculer la moyenne de tout le numpy array, puis diviser chaque exemple de ce jeu de données par l'écart-type. Pour les images, il est plus simple et plus pratique  de diviser uniquement chaque ligne du dataset par 255 (la valeur maximum d'un canal de couleur). Vous vous retrouvez alors avec un numpy array comprenant des nombres entre 0 et 1.\n",
    "\n",
    "Normalisons notre jeu de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_flatten/255.\n",
    "x_test = x_test_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "\n",
    "Les étapes courantes de préprocessing:\n",
    "- Analyser les données en affichant les dimensions et les formes du jeu de données (m_train, m_test, num_px, etc.)\n",
    "- Transformer les jeux de données de façon à ce que chaque exemple devienne un vecteur de dimension (num_px \\* num_px \\* 3, 1)\n",
    "- Normaliser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Architecture générale de l'algorithme d'apprentissage ##\n",
    "\n",
    "C'est le moment de construire un simple algorithme pour reconnaître un chat d'un non-chat d'une image.\n",
    "\n",
    "Vous allez construire une régression logistique, tout en suivant le mindset (l'état d'esprit) d'un réseau de neurones. La figure suivante vous explique pourquoi **la régression logistique** est un réseau de neurones très simple !\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Expression mathématique de l'algorithme**:\n",
    "\n",
    "Pour chaque exemple $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "Le coût est ensuite calculé en additionnant toutes les **losses** (pertes) de chaque exemple.\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Etapes clés**:\n",
    "Dans cet exercice, vous allez traiter les étapes suivantes:\n",
    "    - Initialiser les paramètres du modèle\n",
    "    - Apprendre les paramètres au modèle en minimisant le coût\n",
    "    - Utiliser les paramètres appris pour faire des prédictions (sur le jeu de données de test)\n",
    "    - Analyser les résultats et conclure\n",
    "\n",
    "N'hésitez pas à appeler l'un des assistants si vous avez besoin d'aide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Construire les différentes parties de l'algorithme ## \n",
    "\n",
    "Les étapes principales pour construire un réseau de neurones sont:\n",
    "1. Définir la structure du modèle (comme le nombre de features (fonctionnalités) en entrée)\n",
    "2. Initialiser les paramètres du modèle\n",
    "3. Boucle:\n",
    "    - Calculer la perte actuelle (forward propagation)\n",
    "    - Calculer le gradient courant (backward propagation)\n",
    "    - Mettre à jour les paramètres (descente de gradient)\n",
    "    \n",
    "### 4.1 - Fonctions utiles\n",
    "\n",
    "**Exercise**: \n",
    "En utilisant votre code du workshop dernier (sur Python et numpy), implémenter la fonction sigmoïde. Comme vous l'avez vu dans la figure précédente, vous avez besoin de calculer $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ pour faire des prédictions. Utilisez np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- Un nombre scalaire ou un numpy array\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([-1, 0, 0.5, 2, 3]) = [0.26894142 0.5        0.62245933 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([-1, 0, 0.5, 2, 3]) = \" + str(sigmoid(np.array([-1, 0, 0.5, 2, 3]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "sigmoid([-1, 0, 0.5, 2, 3]) = [0.26894142 0.5        0.62245933 0.88079708 0.95257413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialisation des paramètres\n",
    "\n",
    "**Exercise:** \n",
    "Implémentez l'initialisation des paramètres dans la cellule suivante. Vous devez initialiser w comme un vecteur ne contenant que des 0. Si vous ne savez pas quelle fonction numpy utiliser, renseignez vous sur np.zeros() dans la doc de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    \"\"\"\n",
    "    Cette fonction crée un vecteur de zeros de shape (dim, 1) pour w et initialise b à 0\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size du vecteur w qu'on veut (ou le nombre de paramètres dans ce cas)\n",
    "    \n",
    "    Returns:\n",
    "    w -- vecteur initialisé de shape (dim, 1)\n",
    "    b -- nombre scalaire initialisé correspondant au biais\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    ### Fin du code ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 5\n",
    "w, b = initialize(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "w = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    " \n",
    "b = 0\n",
    "\n",
    "Pour les images, w doit être de la forme (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward et Backward propagation\n",
    "\n",
    "Maintenant que vos paramètres sont initialisés, vous pouvez écrire les étapes de forward et backward propagation pour apprendre les paramètres.\n",
    "\n",
    "\n",
    "**Exercise:** Implémentez la fonction `propagate()` qui va calculer la fonction de coût et son gradient.\n",
    "\n",
    "**Indices**:\n",
    "\n",
    "Forward Propagation:\n",
    "- On vous donne X\n",
    "- Vous calculez $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Vous calculez la fonction de coût: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Voici les deux formules que vous utiliserez: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- les poids, un numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un nombre scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- le vecteur correspondant aux labels (0 si non-chat, 1 si chat) de size (1, nombre d'exemples)\n",
    "    \n",
    "    Return:\n",
    "    cost -- coût\n",
    "    dw -- gradient de la loss de même shape que w\n",
    "    db -- gradient de la loss de même shape que b\n",
    "    \n",
    "    Conseils:\n",
    "    - Ecrivez votre code étape par étape pour la propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Forward Propagation (de X à cost)\n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    # calcule l'activation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    # compute cost\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    # Backward Propagation (pour trouver les gradients)\n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    dw = np.dot(np.dot(1/m, X), (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    ### Fin du code ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.999923  ]\n",
      " [2.39975403]]\n",
      "db = 7.288578855054369e-05\n",
      "cost = 8.800077000774023\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[4.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagation(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  dw   </td>\n",
    "      <td> [[0.999923  ]\n",
    "         [2.39975403]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  db  </td>\n",
    "        <td> 7.288578855054369e-05 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  cost  </td>\n",
    "        <td> 8.800077000774023 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Optimisation\n",
    "\n",
    "- Vous avez initialisé vos paramètres.\n",
    "- Vous savez aussi calculer une fonction de coût et son gradient.\n",
    "- Maintenant, vous voulez mettre à jour les paramètres en utilisant la descente de gradient.\n",
    "\n",
    "\n",
    "**Exercise:** Ecrivez la fonction d'optimisation. Le but est d'apprendre $w$ et $b$ en minimisant la fonction de coût $J$. Pour un paramètre $\\theta$ donné, la formule de mise à jour est $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, où $\\alpha$ correspond au **learning rate** (taux d'apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(w, b, X, Y, n_iterations, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- les poids, numpy array de size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- le vecteur correspondant aux labels (0 si non-chat, 1 si chat) de size (1, nombre d'exemples)\n",
    "    n_iterations -- nombre d'itérations dans la boucle d'optimisation\n",
    "    learning_rate -- le pas d'apprentissage\n",
    "    print_cost -- True pour afficher la loss toutes les 100 fois\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionnaire python contenant les poids w et le biais b\n",
    "    grads -- dictionnaire contenant les gradients de w et de b grâce à la fonction de coût\n",
    "    costs -- list de tous les coûts calculés pendant l'optimisation, cela nous permettra d'afficher la courbe d'apprentissage\n",
    "\n",
    "    Conseils:\n",
    "    Vous aurez besoin d'écrire 2 étapes et d'itérer à travers elles:\n",
    "        1) Calculez le coût et le gradient des paramètres. Utilisez propagation()\n",
    "        2) Updatez les paramètres en utilisant la descente de gradient pour w et b \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        \n",
    "        # Calcul du coût et du gradient (≈ 1-4 lignes de code)\n",
    "        ### Début du code ### \n",
    "        grads, cost = propagation(w, b, X, Y)\n",
    "        ### Fin du code ###\n",
    "        \n",
    "        # Récupérer les dérivées depuis grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update (≈ 2 lignes de code)\n",
    "        ### Début du code ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        ### Fin du code ###\n",
    "        \n",
    "        # Stockez les coûts\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Afficher le coût toutes les 100 itérations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coût après iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 3.11567021]\n",
      " [-0.10995646]]\n",
      "b = 1.9850788219795317\n",
      "dw = [[0.89699851]\n",
      " [2.07122651]]\n",
      "db = 0.09736680112994214\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimization(w, b, X, Y, n_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "w = [[ 3.11567021]\n",
    " [-0.10995646]]\n",
    "\n",
    "b = 1.9850788219795317\n",
    "\n",
    "dw = [[0.89699851]\n",
    " [2.07122651]]\n",
    "\n",
    "db = 0.09736680112994214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** La fonction précedente a permis d'afficher les poids w et le biais b appris. On peut les utiliser pour prédire les labels d'un jeu de données X. Implémentez la fonction `prediction()`. Il y a 2 étapes pour calculer les prédictions:\n",
    "\n",
    "1. Calculer $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convertir les entrées de A en 0 (si activation <= 0.5) ou 1 (si activation > 0.5) et les stocker dans un vecteur `Y_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, b, X):\n",
    "    '''\n",
    "    Arguments:\n",
    "    w -- les poids, numpy array de size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- un numpy array (vecteur) contenant toutes les prédictions (0/1) des exemples de X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Calculez le vecteur \"A\" correspondant aux prédictions si un chat est présent dans l'image\n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convertissez les prédictions A[0,i] en p[0,i]\n",
    "        ### Début du code ### (≈ 4 lignes de code)\n",
    "        Y_prediction[0, i] = (A[0][i] > 0.5)\n",
    "        ### Fin du code ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.2,-2.4],[1.4,2.5,0.6]])\n",
    "print (\"predictions = \" + str(prediction(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             predictions\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.  0.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "\n",
    "Vous avez implémenté plusieurs fonctions qui:\n",
    "- initialise (w, b)\n",
    "- optimise la perte de façon itérative pour apprendre les paramètres (w, b):\n",
    "    - calcul du coût et de son gradient\n",
    "    - mise à jour des paramètres en utilisant la descente de gradient\n",
    "- utilise les (w, b) appris pour prédire les labels d'un jeu de données d'exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Mergez toutes les fonctions dans un modèle unique ##\n",
    "\n",
    "Vous allez maintenant voir comment le modèle global est structuré en intégrant tous les blocs (les fonctions que vous avez implémentées) ensemble, dans le bon ordre.\n",
    "\n",
    "**Exercice**: Implémentez la fonction \"model\". Utilisez les notations suivantes:\n",
    "    - Y_pred_test pour vos prédictions sur le jeu de données de test\n",
    "    - Y_pred_train pour vos prédictions sur le jeu de données de tain\n",
    "    - w, costs, grads pour les résultats de optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, n_iterations = 2000, learning_rate = 0.5, print_cost=False):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    X_train -- jeu de données de train représenté par un numpy array de shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- labels des données de train représenté par un numpy array (vecteur) of shape (1, m_train)\n",
    "    X_test -- jeu de données de test représenté par un numpy array de shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- labels des données de test représenté par un numpy array (vecteur) of shape (1, m_test)\n",
    "    n_iterations -- nombre d'itérations dans la boucle d'optimisation\n",
    "    learning_rate -- le pas d'apprentissage\n",
    "    print_cost -- True pour afficher la loss toutes les 100 fois\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    d -- dictionnaire python contenant toutes les informations du modèle.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ###\n",
    "\n",
    "    # Initialisez les paramètres avec des 0 (≈ 1 ligne de code)\n",
    "    w, b = initialize(X_train.shape[0])\n",
    "\n",
    "    # Descente de gradient (≈ 1 ligne de code)\n",
    "    parameters, grads, costs = propagation(w, b, X_train, Y_train)\n",
    "    \n",
    "    # récupérez les paramètres depuis parameters\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Prédisez les exemples de train et de test (≈ 2 lignes de code)\n",
    "    Y_pred_test = None\n",
    "    Y_pred_train =  None\n",
    "\n",
    "    ### Fin du code ###\n",
    "\n",
    "    # Print les erreurs de train et de test\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_pred_test\": Y_pred_test, \n",
    "         \"Y_pred_train\" : Y_pred_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": n_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la cellule suivante pour entraîner votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.006\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [131]\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, n_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     20\u001b[0m w, b \u001b[38;5;241m=\u001b[39m initialize(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Descente de gradient (≈ 1 ligne de code)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m parameters, grads, costs \u001b[38;5;241m=\u001b[39m propagation(w, b, X_train, Y_train)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# récupérez les paramètres depuis parameters\u001b[39;00m\n\u001b[1;32m     26\u001b[0m w \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "d = model(x_train, y_train, x_test, y_test, n_iterations = 2000, learning_rate = 0.006, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "Coût après iteration 0: 0.693147\n",
    "\n",
    "Coût après iteration 100: 0.649811\n",
    "\n",
    "Coût après iteration 200: 0.538312\n",
    "\n",
    "Coût après iteration 300: 0.439262\n",
    "\n",
    "Coût après iteration 400: 0.349825\n",
    "\n",
    "Coût après iteration 500: 0.278498\n",
    "\n",
    "Coût après iteration 600: 0.249764\n",
    "\n",
    "Coût après iteration 700: 0.231178\n",
    "\n",
    "Coût après iteration 800: 0.215229\n",
    "\n",
    "Coût après iteration 900: 0.201339\n",
    "\n",
    "Coût après iteration 1000: 0.189110\n",
    "\n",
    "Coût après iteration 1100: 0.178249\n",
    "\n",
    "Coût après iteration 1200: 0.168533\n",
    "\n",
    "Coût après iteration 1300: 0.159788\n",
    "\n",
    "Coût après iteration 1400: 0.151873\n",
    "\n",
    "Coût après iteration 1500: 0.144677\n",
    "\n",
    "Coût après iteration 1600: 0.138104\n",
    "\n",
    "Coût après iteration 1700: 0.132079\n",
    "\n",
    "Coût après iteration 1800: 0.126537\n",
    "\n",
    "Coût après iteration 1900: 0.121421\n",
    "\n",
    "train accuracy: 99.52153110047847 %\n",
    "\n",
    "test accuracy: 68.0 %\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Commentaire**: Votre accurary (précision) de train est très proche de 100%. C'est très bien, votre modèle est fonctionnel et sait très bien reconnaître les chats du jeu de données de train. Cependant, l'accuracy de test est de 68%. C'est n'est pas si mal pour le modèle simple qu'on a construit étant donné le petit jeu de données fourni, mais ne vous inquiéter pas, nous construirons un meilleur modèle dans un futur workshop.\n",
    "\n",
    "Vous avez aussi remarqué que le modèle à surinterpréter (overfit) les données de train. Nous verrons plus tard des méthodes pour réduire la surinterprétation (en utilisant la régularisation par exemple). Utilisez le code ci-dessous pour afficher la fonction de coût et ses gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (par centaine)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Interprétation**:\n",
    "Vous pouvez voir que le coût décroît. Cela montre que les paramètres sont en cours d'apprentissage. Cependant, vous voyez aussi que le modèle s'entraîne trop sur le jeu de données de train. Essayez d'augmenter le nombre d'itérations dans la cellule au dessus et re-exécutez les cellules. Vous devriez voir que la précision du jeu de donnée d'entraînement augmente mais que celui de test baisse. C'est ce qu'on appelle l'overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques sources utiles:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
